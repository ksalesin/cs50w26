<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title></title>
    <link rel="stylesheet" href="/styles.css">
    <link rel="stylesheet" href="/github-markdown.css">
</head>
<body>
    <div class="markdown-body nav">
        <h3><a href="/" id="cs50">CS 50: Software Design and Implementation</a></h3>
        <ul>
            <li><a href="/syllabus">Syllabus</a></li>
            <li><a href="/schedule">Schedule</a></li>
            <li><a href="/style">Style Guide</a></li>
            <li><a href="/resources">Resources</a></li>
            <!-- <li><a href="/submit.html">Submit</a></li> -->
            <li id="knowledge-base"><a href="http://plink.cs.dartmouth.edu:3000/" target="_blank">Knowledge Base</a></li>
        </ul>
    </div>
    <div class="markdown-body content">
        <h1>CS50 TSE Crawler</h1>
<h2>Design Spec</h2>
<p>In this document we reference the <a href="REQUIREMENTS.md">Requirements Specification</a> and focus on the implementation-independent design decisions.
The knowledge unit noted that an <a href="https://github.com/CS50DartmouthFA2025/home/blob/main/knowledge/units/design.md#design-spec">design spec</a> may include many topics; not all are relevant to the TSE or the Crawler.
Here we focus on the core subset:</p>
<ul>
<li>User interface</li>
<li>Inputs and outputs</li>
<li>Functional decomposition into modules</li>
<li>Pseudo code (plain English-like language) for logic/algorithmic flow</li>
<li>Major data structures</li>
<li>Testing plan</li>
</ul>
<h2>User interface</h2>
<p>As described in the <a href="REQUIREMENTS.md">Requirements Spec</a>, the crawler's only interface with the user is on the command-line; it must always have three arguments.</p>
<pre><code class="language-bash">$ crawler seedURL pageDirectory maxDepth
</code></pre>
<p>For example, to crawl one of the CS50 test sites, store the pages found in a subdirectory <code>data</code> in the current directory, and to search only depths 0, 1, and 2, use this command line:</p>
<pre><code class="language-bash">$ mkdir ../data/letters
$ ./crawler http://cs50tse.cs.dartmouth.edu/tse/letters/index.html ../data/letters 2
</code></pre>
<h2>Inputs and outputs</h2>
<p><em>Input:</em> there are no file inputs; there are command-line parameters described above.</p>
<p><em>Output:</em> Per the requirements spec, Crawler will save each explored webpage to a file, one webpage per file, using a unique <code>documentID</code> as the file name.  For example,
the top file of the website would have <code>documentID</code> 1, the next webpage access from a link on that top page would be <code>documentID</code> 2, and so on.
Within each of these files, crawler writes:</p>
<ul>
<li>the full page URL on the first line,</li>
<li>the depth of the page (where the <code>seedURL</code> is considered to be depth 0) on the second line,</li>
<li>the page contents (i.e., the HTML code), beginning on the third line.</li>
</ul>
<h2>Functional decomposition into modules</h2>
<p>We anticipate the following modules or functions:</p>
<ol>
<li><em>main</em>, which parses arguments and initializes other modules</li>
<li><em>crawler</em>, which loops over pages to explore, until the list is exhausted</li>
<li><em>pagefetcher</em>, which fetches a page from a URL</li>
<li><em>pagescanner</em>, which extracts URLs from a page and processes each one</li>
<li><em>pagesaver</em>, which outputs a page to the the appropriate file</li>
</ol>
<p>And some helper modules that provide data structures:</p>
<ol>
<li><em>bag</em> of pages we have yet to explore</li>
<li><em>hashtable</em> of URLs we've seen so far</li>
</ol>
<h2>Pseudo code for logic/algorithmic flow</h2>
<p>The crawler will run as follows:</p>
<p>parse the command line, validate parameters, initialize other modules
add seedURL to the bag of webpages to crawl, marked with depth=0
add seedURL to the hashtable of URLs seen so far
while there are more webpages in the bag:
extract a webpage (URL,depth) item from the bag
pause for one second
use pagefetcher to retrieve a webpage for that URL
use pagesaver to write the webpage to the pageDirectory with a unique document ID
if the webpage depth is &lt; maxDepth, explore the webpage to find the links it contains:
use pagescanner to parse the webpage to extract all its embedded URLs
for each extracted URL:
normalize the URL (per requirements spec)
if that URL is internal (per requirements spec):
try to insert that URL into the <em>hashtable</em> of URLs seen;
if it was already in the table, do nothing;
if it was added to the table:
create a new webpage for that URL, marked with depth+1
add that new webpage to the bag of webpages to be crawled</p>
<p>Notice that our pseudocode says nothing about the order in which it crawls webpages.
Recall that our <em>bag</em> abstract data structure explicitly denies any promise about the order of items removed from a bag.
That's ok.
The result may or may not be a Breadth-First Search, but for the crawler we don't care about the order as long as we explore everything within the <code>maxDepth</code> neighborhood.</p>
<p>The crawler completes and exits when it has nothing left in its <em>bag</em> - no more pages to be crawled.
The maxDepth parameter indirectly determines the number of pages that the crawler will retrieve.</p>
<h2>Major data structures</h2>
<p>Helper modules provide all the data structures we need:</p>
<ul>
<li><em>bag</em> of webpage (URL, depth) structures</li>
<li><em>hashtable</em> of URLs</li>
<li><em>webpage</em> contains all the data read for a given webpage, plus the URL and the depth at which it was fetched</li>
</ul>
<h2>Testing plan</h2>
<p>We've established a '<a href="http://cs50tse.cs.dartmouth.edu/tse/">playground</a>' for CS50 crawlers to explore.</p>
<p>A sampling of tests that should be run:</p>
<ol>
<li>
<p>Test the program with various forms of incorrect command-line arguments to ensure that its command-line parsing, and validation of those parameters, works correctly.</p>
</li>
<li>
<p>Crawl a simple, closed set of cross-linked web pages like <a href="http://cs50tse.cs.dartmouth.edu/tse/letters/">letters</a>, at depths 0, 1, 2, or more.
Verify that the files created match expectations.</p>
</li>
<li>
<p>Repeat with a different seed page in that same site.
If the site is indeed a graph, with cycles, there should be several interesting starting points.</p>
</li>
<li>
<p>Point the crawler at one of our bigger playgrounds.
Explore at depths 0, 1, 2, from various starting pages.
(It takes a long time to run at depth 2 or higher!)
Verify that the files created match expectations.</p>
</li>
<li>
<p>When you are confident that your crawler runs well, test it with a greater depth - but be ready to kill it if it seems to be running amok.</p>
</li>
</ol>

    </div>
</body>
</html>